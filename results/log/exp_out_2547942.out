INFO flwr 2023-04-04 20:38:28,076 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2023-04-04 20:38:30,721	INFO worker.py:1529 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
INFO flwr 2023-04-04 20:38:33,716 | app.py:179 | Flower VCE: Ray initialized with resources: {'node:145.136.62.20': 1.0, 'CPU': 72.0, 'GPU': 1.0, 'object_store_memory': 158511900672.0, 'memory': 359861101568.0, 'accelerator_type:A100': 1.0}
INFO flwr 2023-04-04 20:38:33,716 | server.py:86 | Initializing global parameters
INFO flwr 2023-04-04 20:38:33,716 | server.py:266 | Using initial parameters provided by strategy
INFO flwr 2023-04-04 20:38:33,716 | server.py:88 | Evaluating initial parameters
INFO flwr 2023-04-04 20:38:33,716 | server.py:101 | FL starting
DEBUG flwr 2023-04-04 20:38:33,716 | server.py:215 | fit_round 1: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:39:10,483 | server.py:229 | fit_round 1 received 2 results and 0 failures
WARNING flwr 2023-04-04 20:39:10,646 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
DEBUG flwr 2023-04-04 20:39:10,646 | server.py:165 | evaluate_round 1: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:39:22,444 | server.py:179 | evaluate_round 1 received 2 results and 0 failures
WARNING flwr 2023-04-04 20:39:22,444 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
DEBUG flwr 2023-04-04 20:39:22,444 | server.py:215 | fit_round 2: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:39:58,216 | server.py:229 | fit_round 2 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:39:58,304 | server.py:165 | evaluate_round 2: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:40:10,098 | server.py:179 | evaluate_round 2 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:40:10,098 | server.py:215 | fit_round 3: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:40:46,317 | server.py:229 | fit_round 3 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:40:46,407 | server.py:165 | evaluate_round 3: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:40:58,494 | server.py:179 | evaluate_round 3 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:40:58,494 | server.py:215 | fit_round 4: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:41:35,285 | server.py:229 | fit_round 4 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:41:35,371 | server.py:165 | evaluate_round 4: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:41:47,556 | server.py:179 | evaluate_round 4 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:41:47,556 | server.py:215 | fit_round 5: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:42:23,816 | server.py:229 | fit_round 5 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:42:23,905 | server.py:165 | evaluate_round 5: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:42:36,397 | server.py:179 | evaluate_round 5 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:42:36,397 | server.py:215 | fit_round 6: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:43:12,821 | server.py:229 | fit_round 6 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:43:12,903 | server.py:165 | evaluate_round 6: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:43:25,621 | server.py:179 | evaluate_round 6 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:43:25,621 | server.py:215 | fit_round 7: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:43:30,257 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1961013, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:30,851 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1961012, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:30,851 | server.py:229 | fit_round 7 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:43:30,852 | server.py:165 | evaluate_round 7: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:43:34,083 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=1961302, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:34,115 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=1961303, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:34,115 | server.py:179 | evaluate_round 7 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:43:34,115 | server.py:215 | fit_round 8: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:43:37,381 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1961521, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:37,396 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1961520, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:37,396 | server.py:229 | fit_round 8 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:43:37,396 | server.py:165 | evaluate_round 8: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:43:40,722 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=1961738, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:40,737 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=1961737, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:40,737 | server.py:179 | evaluate_round 8 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:43:40,737 | server.py:215 | fit_round 9: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:43:44,003 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1962079, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:44,019 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1962086, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:44,019 | server.py:229 | fit_round 9 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:43:44,019 | server.py:165 | evaluate_round 9: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:43:47,322 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=1970589, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:47,330 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=1970605, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:47,330 | server.py:179 | evaluate_round 9 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:43:47,330 | server.py:215 | fit_round 10: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:43:50,616 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1985026, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:50,631 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1985009, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:50,631 | server.py:229 | fit_round 10 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:43:50,631 | server.py:165 | evaluate_round 10: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:43:53,929 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=2002047, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:53,936 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=2002070, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:43:53,936 | server.py:179 | evaluate_round 10 received 0 results and 2 failures
INFO flwr 2023-04-04 20:43:53,936 | server.py:144 | FL finished in 320.2202388870064
INFO flwr 2023-04-04 20:43:53,937 | app.py:202 | app_fit: losses_distributed [(1, 2.295776384162903), (2, 1.5720325548171996), (3, 1.2908760185480117), (4, 1.1807650949001312), (5, 1.0386123226046562), (6, 1.093576066017151)]
INFO flwr 2023-04-04 20:43:53,937 | app.py:203 | app_fit: metrics_distributed {}
INFO flwr 2023-04-04 20:43:53,937 | app.py:204 | app_fit: losses_centralized []
INFO flwr 2023-04-04 20:43:53,937 | app.py:205 | app_fit: metrics_centralized {}
Files already downloaded and verified
Files already downloaded and verified
[2m[36m(launch_and_fit pid=1948094)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1948093)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1948094)[0m Epoch 1 : loss 2.248152017593384, acc 0.2789333333333333
[2m[36m(launch_and_fit pid=1948093)[0m Epoch 1 : loss 2.2267165184020996, acc 0.27542222222222223
[2m[36m(launch_and_fit pid=1948094)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=1948093)[0m Getting parameters from Client 0
[2m[36m(launch_and_evaluate pid=1954367)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=1954366)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=1954769)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1954770)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1954769)[0m Epoch 1 : loss 1.7812483310699463, acc 0.35631111111111113
[2m[36m(launch_and_fit pid=1954769)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=1954770)[0m Epoch 1 : loss 1.7786210775375366, acc 0.3607111111111111
[2m[36m(launch_and_fit pid=1954770)[0m Getting parameters from Client 0
[2m[36m(launch_and_evaluate pid=1955259)[0m Evaluating Client 0
[2m[36m(launch_and_evaluate pid=1955260)[0m Evaluating Client 1
[2m[36m(launch_and_fit pid=1955518)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1955517)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1955518)[0m Epoch 1 : loss 1.5868067741394043, acc 0.43444444444444447
[2m[36m(launch_and_fit pid=1955517)[0m Epoch 1 : loss 1.5972654819488525, acc 0.42933333333333334
[2m[36m(launch_and_fit pid=1955518)[0m Getting parameters from Client 0
[2m[36m(launch_and_fit pid=1955517)[0m Getting parameters from Client 1
[2m[36m(launch_and_evaluate pid=1956276)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=1956277)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=1956593)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1956594)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1956593)[0m Epoch 1 : loss 1.431504487991333, acc 0.4915111111111111
[2m[36m(launch_and_fit pid=1956594)[0m Epoch 1 : loss 1.4333945512771606, acc 0.4960888888888889
[2m[36m(launch_and_fit pid=1956593)[0m Getting parameters from Client 0
[2m[36m(launch_and_fit pid=1956594)[0m Getting parameters from Client 1
[2m[36m(launch_and_evaluate pid=1957559)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=1957560)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=1957905)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1957906)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1957905)[0m Epoch 1 : loss 1.3074876070022583, acc 0.5366222222222222
[2m[36m(launch_and_fit pid=1957906)[0m Epoch 1 : loss 1.2716879844665527, acc 0.5502222222222222
[2m[36m(launch_and_fit pid=1957905)[0m Getting parameters from Client 0
[2m[36m(launch_and_fit pid=1957906)[0m Getting parameters from Client 1
[2m[36m(launch_and_evaluate pid=1958987)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=1958988)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=1959380)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1959379)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1959380)[0m Epoch 1 : loss 1.1805016994476318, acc 0.5908
[2m[36m(launch_and_fit pid=1959379)[0m Epoch 1 : loss 1.3244725465774536, acc 0.5332888888888889
[2m[36m(launch_and_fit pid=1959380)[0m Getting parameters from Client 0
[2m[36m(launch_and_fit pid=1959379)[0m Getting parameters from Client 1
[2m[36m(launch_and_evaluate pid=1960366)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=1960367)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=1961013)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1961012)[0m Fitting Client 0
[2m[36m(launch_and_evaluate pid=1961303)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=1961302)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=1961520)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1961521)[0m Fitting Client 0
[2m[36m(launch_and_evaluate pid=1961738)[0m Evaluating Client 0
[2m[36m(launch_and_evaluate pid=1961737)[0m Evaluating Client 1
[2m[36m(launch_and_fit pid=1962079)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1962086)[0m Fitting Client 1
[2m[36m(launch_and_evaluate pid=1970605)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=1970589)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=1985009)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1985026)[0m Fitting Client 1
[2m[36m(launch_and_evaluate pid=2002070)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=2002047)[0m Evaluating Client 0

JOB STATISTICS
==============
Job ID: 2547942
Cluster: snellius
User/Group: sunnys/sunnys
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 01:47:42 core-walltime
Job Wall-clock time: 00:05:59
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 0.00 MB (0.00 MB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
