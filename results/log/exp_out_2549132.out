INFO flwr 2023-04-05 10:41:53,328 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2023-04-05 10:41:56,040	INFO worker.py:1529 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
INFO flwr 2023-04-05 10:41:59,191 | app.py:179 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:A100': 1.0, 'object_store_memory': 145086208819.0, 'memory': 328534487245.0, 'CPU': 72.0, 'node:145.136.62.33': 1.0}
INFO flwr 2023-04-05 10:41:59,192 | server.py:86 | Initializing global parameters
INFO flwr 2023-04-05 10:41:59,192 | server.py:266 | Using initial parameters provided by strategy
INFO flwr 2023-04-05 10:41:59,192 | server.py:88 | Evaluating initial parameters
INFO flwr 2023-04-05 10:41:59,192 | server.py:101 | FL starting
DEBUG flwr 2023-04-05 10:41:59,192 | server.py:215 | fit_round 1: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:43:09,036 | server.py:229 | fit_round 1 received 2 results and 0 failures
WARNING flwr 2023-04-05 10:43:09,195 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
DEBUG flwr 2023-04-05 10:43:09,196 | server.py:165 | evaluate_round 1: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:43:28,607 | server.py:179 | evaluate_round 1 received 2 results and 0 failures
WARNING flwr 2023-04-05 10:43:28,607 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
DEBUG flwr 2023-04-05 10:43:28,608 | server.py:215 | fit_round 2: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:44:35,690 | server.py:229 | fit_round 2 received 2 results and 0 failures
DEBUG flwr 2023-04-05 10:44:35,776 | server.py:165 | evaluate_round 2: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:44:55,562 | server.py:179 | evaluate_round 2 received 2 results and 0 failures
DEBUG flwr 2023-04-05 10:44:55,562 | server.py:215 | fit_round 3: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:46:03,291 | server.py:229 | fit_round 3 received 2 results and 0 failures
DEBUG flwr 2023-04-05 10:46:03,373 | server.py:165 | evaluate_round 3: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:46:23,217 | server.py:179 | evaluate_round 3 received 2 results and 0 failures
DEBUG flwr 2023-04-05 10:46:23,218 | server.py:215 | fit_round 4: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:47:31,143 | server.py:229 | fit_round 4 received 2 results and 0 failures
DEBUG flwr 2023-04-05 10:47:31,224 | server.py:165 | evaluate_round 4: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:47:51,685 | server.py:179 | evaluate_round 4 received 2 results and 0 failures
DEBUG flwr 2023-04-05 10:47:51,685 | server.py:215 | fit_round 5: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:49:00,543 | server.py:229 | fit_round 5 received 2 results and 0 failures
DEBUG flwr 2023-04-05 10:49:00,633 | server.py:165 | evaluate_round 5: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:49:20,805 | server.py:179 | evaluate_round 5 received 2 results and 0 failures
DEBUG flwr 2023-04-05 10:49:20,806 | server.py:215 | fit_round 6: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:50:29,025 | server.py:229 | fit_round 6 received 2 results and 0 failures
DEBUG flwr 2023-04-05 10:50:29,120 | server.py:165 | evaluate_round 6: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:50:49,297 | server.py:179 | evaluate_round 6 received 2 results and 0 failures
DEBUG flwr 2023-04-05 10:50:49,297 | server.py:215 | fit_round 7: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:51:26,468 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3226982, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:26,468 | server.py:229 | fit_round 7 received 1 results and 1 failures
DEBUG flwr 2023-04-05 10:51:26,515 | server.py:165 | evaluate_round 7: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:51:30,582 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=3227080, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:32,813 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=3227175, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:32,813 | server.py:179 | evaluate_round 7 received 0 results and 2 failures
DEBUG flwr 2023-04-05 10:51:32,814 | server.py:215 | fit_round 8: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:51:36,869 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3227275, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:39,109 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3227398, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:39,109 | server.py:229 | fit_round 8 received 0 results and 2 failures
DEBUG flwr 2023-04-05 10:51:39,109 | server.py:165 | evaluate_round 8: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:51:43,148 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=3227497, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:45,412 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=3227592, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:45,412 | server.py:179 | evaluate_round 8 received 0 results and 2 failures
DEBUG flwr 2023-04-05 10:51:45,413 | server.py:215 | fit_round 9: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:51:49,407 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3227712, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:51,653 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3227787, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:51,653 | server.py:229 | fit_round 9 received 0 results and 2 failures
DEBUG flwr 2023-04-05 10:51:51,653 | server.py:165 | evaluate_round 9: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:51:55,667 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=3227910, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:58,081 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=3228033, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:51:58,081 | server.py:179 | evaluate_round 9 received 0 results and 2 failures
DEBUG flwr 2023-04-05 10:51:58,081 | server.py:215 | fit_round 10: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:52:02,119 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3228304, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:52:04,359 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3228389, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:52:04,360 | server.py:229 | fit_round 10 received 0 results and 2 failures
DEBUG flwr 2023-04-05 10:52:04,360 | server.py:165 | evaluate_round 10: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-05 10:52:08,389 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=3228514, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:52:10,612 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=3228758, ip=145.136.62.33)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-05 10:52:10,613 | server.py:179 | evaluate_round 10 received 0 results and 2 failures
INFO flwr 2023-04-05 10:52:10,613 | server.py:144 | FL finished in 611.4210651800095
INFO flwr 2023-04-05 10:52:10,613 | app.py:202 | app_fit: losses_distributed [(1, 2.3128878734588625), (2, 1.5102000732421874), (3, 1.2583021869182587), (4, 1.1959421445608138), (5, 1.0798943238258363), (6, 0.9697125484228134)]
INFO flwr 2023-04-05 10:52:10,613 | app.py:203 | app_fit: metrics_distributed {}
INFO flwr 2023-04-05 10:52:10,613 | app.py:204 | app_fit: losses_centralized []
INFO flwr 2023-04-05 10:52:10,613 | app.py:205 | app_fit: metrics_centralized {}
Files already downloaded and verified
Files already downloaded and verified
[2m[36m(launch_and_fit pid=3218208)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3218208)[0m Epoch 1 : loss 2.2364590167999268, acc 0.28155555555555556
[2m[36m(launch_and_fit pid=3218208)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=3218566)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=3218566)[0m Epoch 1 : loss 2.221341371536255, acc 0.28102222222222223
[2m[36m(launch_and_fit pid=3218566)[0m Getting parameters from Client 0
[2m[36m(launch_and_evaluate pid=3219140)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=3219221)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=3219311)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3219311)[0m Epoch 1 : loss 1.7973328828811646, acc 0.35431111111111113
[2m[36m(launch_and_fit pid=3219311)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=3219640)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=3219640)[0m Epoch 1 : loss 1.7938040494918823, acc 0.3531111111111111
[2m[36m(launch_and_fit pid=3219640)[0m Getting parameters from Client 0
[2m[36m(launch_and_evaluate pid=3220183)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=3220275)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=3220407)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=3220407)[0m Epoch 1 : loss 1.576541543006897, acc 0.4356
[2m[36m(launch_and_fit pid=3220407)[0m Getting parameters from Client 0
[2m[36m(launch_and_fit pid=3220880)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3220880)[0m Epoch 1 : loss 1.5508811473846436, acc 0.4464
[2m[36m(launch_and_fit pid=3220880)[0m Getting parameters from Client 1
[2m[36m(launch_and_evaluate pid=3221263)[0m Evaluating Client 0
[2m[36m(launch_and_evaluate pid=3221670)[0m Evaluating Client 1
[2m[36m(launch_and_fit pid=3221781)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3221781)[0m Epoch 1 : loss 1.432855486869812, acc 0.49262222222222224
[2m[36m(launch_and_fit pid=3221781)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=3222196)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=3222196)[0m Epoch 1 : loss 1.4375152587890625, acc 0.4908888888888889
[2m[36m(launch_and_fit pid=3222196)[0m Getting parameters from Client 0
[2m[36m(launch_and_evaluate pid=3222617)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=3222905)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=3223043)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3223043)[0m Epoch 1 : loss 1.2831844091415405, acc 0.5525777777777777
[2m[36m(launch_and_fit pid=3223043)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=3223636)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=3223636)[0m Epoch 1 : loss 1.2811558246612549, acc 0.5506222222222222
[2m[36m(launch_and_fit pid=3223636)[0m Getting parameters from Client 0
[2m[36m(launch_and_evaluate pid=3224114)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=3224628)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=3224837)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3224837)[0m Epoch 1 : loss 1.1649377346038818, acc 0.5951111111111111
[2m[36m(launch_and_fit pid=3224837)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=3225179)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=3225179)[0m Epoch 1 : loss 1.1786824464797974, acc 0.5876444444444444
[2m[36m(launch_and_fit pid=3225179)[0m Getting parameters from Client 0
[2m[36m(launch_and_evaluate pid=3225836)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=3226137)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=3226309)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3226309)[0m Epoch 1 : loss 1.083058476448059, acc 0.6251111111111111
[2m[36m(launch_and_fit pid=3226309)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=3226982)[0m Fitting Client 0
[2m[36m(launch_and_evaluate pid=3227080)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=3227175)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=3227275)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3227398)[0m Fitting Client 0
[2m[36m(launch_and_evaluate pid=3227497)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=3227592)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=3227712)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=3227787)[0m Fitting Client 1
[2m[36m(launch_and_evaluate pid=3227910)[0m Evaluating Client 0
[2m[36m(launch_and_evaluate pid=3228033)[0m Evaluating Client 1
[2m[36m(launch_and_fit pid=3228304)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3228389)[0m Fitting Client 0
[2m[36m(launch_and_evaluate pid=3228514)[0m Evaluating Client 0
[2m[36m(launch_and_evaluate pid=3228758)[0m Evaluating Client 1

JOB STATISTICS
==============
Job ID: 2549132
Cluster: snellius
User/Group: sunnys/sunnys
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:13:30 core-walltime
Job Wall-clock time: 00:10:45
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 0.00 MB (0.00 MB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
