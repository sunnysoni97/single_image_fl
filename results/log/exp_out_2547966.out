INFO flwr 2023-04-04 20:49:27,586 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2023-04-04 20:49:29,504	INFO worker.py:1529 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
INFO flwr 2023-04-04 20:49:32,455 | app.py:179 | Flower VCE: Ray initialized with resources: {'node:145.136.62.20': 1.0, 'memory': 359995068621.0, 'object_store_memory': 158569315123.0, 'GPU': 1.0, 'CPU': 72.0, 'accelerator_type:A100': 1.0}
INFO flwr 2023-04-04 20:49:32,456 | server.py:86 | Initializing global parameters
INFO flwr 2023-04-04 20:49:32,456 | server.py:266 | Using initial parameters provided by strategy
INFO flwr 2023-04-04 20:49:32,456 | server.py:88 | Evaluating initial parameters
INFO flwr 2023-04-04 20:49:32,456 | server.py:101 | FL starting
DEBUG flwr 2023-04-04 20:49:32,456 | server.py:215 | fit_round 1: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:50:09,300 | server.py:229 | fit_round 1 received 2 results and 0 failures
WARNING flwr 2023-04-04 20:50:09,451 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
DEBUG flwr 2023-04-04 20:50:09,451 | server.py:165 | evaluate_round 1: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:50:21,341 | server.py:179 | evaluate_round 1 received 2 results and 0 failures
WARNING flwr 2023-04-04 20:50:21,341 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
DEBUG flwr 2023-04-04 20:50:21,341 | server.py:215 | fit_round 2: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:50:56,998 | server.py:229 | fit_round 2 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:50:57,085 | server.py:165 | evaluate_round 2: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:51:08,958 | server.py:179 | evaluate_round 2 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:51:08,958 | server.py:215 | fit_round 3: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:51:45,635 | server.py:229 | fit_round 3 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:51:45,716 | server.py:165 | evaluate_round 3: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:51:57,844 | server.py:179 | evaluate_round 3 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:51:57,844 | server.py:215 | fit_round 4: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:52:34,443 | server.py:229 | fit_round 4 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:52:34,531 | server.py:165 | evaluate_round 4: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:52:46,752 | server.py:179 | evaluate_round 4 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:52:46,752 | server.py:215 | fit_round 5: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:53:23,812 | server.py:229 | fit_round 5 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:53:23,907 | server.py:165 | evaluate_round 5: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:53:36,467 | server.py:179 | evaluate_round 5 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:53:36,467 | server.py:215 | fit_round 6: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:54:13,156 | server.py:229 | fit_round 6 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:54:13,238 | server.py:165 | evaluate_round 6: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:54:25,969 | server.py:179 | evaluate_round 6 received 2 results and 0 failures
DEBUG flwr 2023-04-04 20:54:25,969 | server.py:215 | fit_round 7: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:54:31,440 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2035221, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 39.45 GiB total capacity; 2.62 MiB already allocated; 36.81 MiB free; 4.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
DEBUG flwr 2023-04-04 20:54:31,441 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2035222, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 31, in train_model
    outputs = model(images)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
DEBUG flwr 2023-04-04 20:54:31,442 | server.py:229 | fit_round 7 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:54:31,442 | server.py:165 | evaluate_round 7: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:54:34,734 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=2035444, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:34,775 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=2035445, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:34,775 | server.py:179 | evaluate_round 7 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:54:34,775 | server.py:215 | fit_round 8: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:54:38,019 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2035742, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:38,026 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2035741, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:38,027 | server.py:229 | fit_round 8 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:54:38,027 | server.py:165 | evaluate_round 8: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:54:41,327 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=2035963, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:41,342 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=2035962, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:41,342 | server.py:179 | evaluate_round 8 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:54:41,342 | server.py:215 | fit_round 9: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:54:44,612 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2036183, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:44,628 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2036182, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:44,628 | server.py:229 | fit_round 9 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:54:44,628 | server.py:165 | evaluate_round 9: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:54:47,852 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=2036494, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:47,867 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=2036493, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:47,867 | server.py:179 | evaluate_round 9 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:54:47,867 | server.py:215 | fit_round 10: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:54:51,101 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2036723, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:51,116 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2036722, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 258, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 76, in fit
    new_params, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 21, in train_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:51,116 | server.py:229 | fit_round 10 received 0 results and 2 failures
DEBUG flwr 2023-04-04 20:54:51,116 | server.py:165 | evaluate_round 10: strategy sampled 2 clients (out of 2)
DEBUG flwr 2023-04-04 20:54:54,404 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=2036943, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:54,440 | ray_client_proxy.py:104 | [36mray::launch_and_evaluate()[39m (pid=2036942, ip=145.136.62.20)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 160, in launch_and_evaluate
    return maybe_call_evaluate(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 205, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/app.py", line 282, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client.py", line 84, in evaluate
    val_res = test_model(self.model_name, self.model_n_classes,
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/common.py", line 15, in test_model
    model.to(DEVICE)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-04-04 20:54:54,440 | server.py:179 | evaluate_round 10 received 0 results and 2 failures
INFO flwr 2023-04-04 20:54:54,440 | server.py:144 | FL finished in 321.9841980170022
INFO flwr 2023-04-04 20:54:54,440 | app.py:202 | app_fit: losses_distributed [(1, 2.324565991973877), (2, 1.5547033671855925), (3, 1.2698662650585175), (4, 1.149281194663048), (5, 1.0671315070867538), (6, 0.9641370939850807)]
INFO flwr 2023-04-04 20:54:54,440 | app.py:203 | app_fit: metrics_distributed {}
INFO flwr 2023-04-04 20:54:54,440 | app.py:204 | app_fit: losses_centralized []
INFO flwr 2023-04-04 20:54:54,441 | app.py:205 | app_fit: metrics_centralized {}
Files already downloaded and verified
Files already downloaded and verified
[2m[36m(launch_and_fit pid=2014619)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=2014618)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2014619)[0m Epoch 1 : loss 2.1808955669403076, acc 0.28555555555555556
[2m[36m(launch_and_fit pid=2014618)[0m Epoch 1 : loss 2.2121429443359375, acc 0.2707555555555556
[2m[36m(launch_and_fit pid=2014619)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=2014618)[0m Getting parameters from Client 0
[2m[36m(launch_and_evaluate pid=2014967)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=2014968)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=2015446)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2015447)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=2015447)[0m Epoch 1 : loss 1.807350754737854, acc 0.34902222222222223
[2m[36m(launch_and_fit pid=2015446)[0m Epoch 1 : loss 1.820743203163147, acc 0.34346666666666664
[2m[36m(launch_and_fit pid=2015447)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=2015446)[0m Getting parameters from Client 0
[2m[36m(launch_and_evaluate pid=2015870)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=2015869)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=2016119)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2016120)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=2016119)[0m Epoch 1 : loss 1.6008492708206177, acc 0.4252444444444444
[2m[36m(launch_and_fit pid=2016120)[0m Epoch 1 : loss 1.5601048469543457, acc 0.4407111111111111
[2m[36m(launch_and_fit pid=2016119)[0m Getting parameters from Client 0
[2m[36m(launch_and_fit pid=2016120)[0m Getting parameters from Client 1
[2m[36m(launch_and_evaluate pid=2016975)[0m Evaluating Client 0
[2m[36m(launch_and_evaluate pid=2016976)[0m Evaluating Client 1
[2m[36m(launch_and_fit pid=2017264)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2017265)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=2017265)[0m Epoch 1 : loss 1.3995281457901, acc 0.5035111111111111
[2m[36m(launch_and_fit pid=2017264)[0m Epoch 1 : loss 1.4485509395599365, acc 0.4844888888888889
[2m[36m(launch_and_fit pid=2017264)[0m Getting parameters from Client 0
[2m[36m(launch_and_fit pid=2017265)[0m Getting parameters from Client 1
[2m[36m(launch_and_evaluate pid=2018435)[0m Evaluating Client 0
[2m[36m(launch_and_evaluate pid=2018436)[0m Evaluating Client 1
[2m[36m(launch_and_fit pid=2018907)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2018908)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=2018907)[0m Epoch 1 : loss 1.3092268705368042, acc 0.5419555555555555
[2m[36m(launch_and_fit pid=2018907)[0m Getting parameters from Client 0
[2m[36m(launch_and_fit pid=2018908)[0m Epoch 1 : loss 1.2737187147140503, acc 0.5519555555555555
[2m[36m(launch_and_fit pid=2018908)[0m Getting parameters from Client 1
[2m[36m(launch_and_evaluate pid=2025791)[0m Evaluating Client 0
[2m[36m(launch_and_evaluate pid=2025794)[0m Evaluating Client 1
[2m[36m(launch_and_fit pid=2029210)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=2029209)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2029210)[0m Epoch 1 : loss 1.1839468479156494, acc 0.5834666666666667
[2m[36m(launch_and_fit pid=2029210)[0m Getting parameters from Client 1
[2m[36m(launch_and_fit pid=2029209)[0m Epoch 1 : loss 1.193467378616333, acc 0.5845777777777778
[2m[36m(launch_and_fit pid=2029209)[0m Getting parameters from Client 0
[2m[36m(launch_and_evaluate pid=2034570)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=2034571)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=2035222)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=2035221)[0m Fitting Client 0
[2m[36m(launch_and_evaluate pid=2035444)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=2035445)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=2035741)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2035742)[0m Fitting Client 1
[2m[36m(launch_and_evaluate pid=2035963)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=2035962)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=2036182)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2036183)[0m Fitting Client 1
[2m[36m(launch_and_evaluate pid=2036493)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=2036494)[0m Evaluating Client 0
[2m[36m(launch_and_fit pid=2036723)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2036722)[0m Fitting Client 1
[2m[36m(launch_and_evaluate pid=2036943)[0m Evaluating Client 1
[2m[36m(launch_and_evaluate pid=2036942)[0m Evaluating Client 0

JOB STATISTICS
==============
Job ID: 2547966
Cluster: snellius
User/Group: sunnys/sunnys
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 01:45:36 core-walltime
Job Wall-clock time: 00:05:52
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 0.00 MB (0.00 MB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
