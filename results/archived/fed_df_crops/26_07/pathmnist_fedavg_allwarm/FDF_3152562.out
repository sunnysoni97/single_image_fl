-----BEGIN-----
-----SETTINGS-----
STRATEGY:feddf
MODEL_NAME:resnet8
NUM_CLIENTS:20
NUM_ROUNDS:30
FRACTION_FIT:0.4
FRACTION_EVALUATE:0.0
DATASET_NAME:pathmnist
PARTITION_ALPHA:10.0
PARTITION_VAL_RATIO:0.1
BATCH_SIZE:512
LOCAL_EPOCHS:40
LOCAL_LR:0.05
DISTILL_BATCH_SIZE:512
SERVER_LR:0.005
SERVER_STEPS:500
SERVER_EARLY_STEPS:1000
USE_EARLY_STOPPING:False
USE_ADAPTIVE_LR:False
SEED:127
CUDA_DETERMINISTIC:False
USE_CROPS:True
IMG_NAME:colonpath_sample.jpg
DISTILL_DATASET:cifar100
DISTILL_ALPHA:1.0
NUM_DISTILL_IMAGES:10000
DISTILL_TRANSFORMS:v0
WARM_START:True
WARM_START_ROUNDS:1
WARM_START_INTERVAL:1
KMEANS_N_CLUSTER:50
KMEANS_HEURISTICS:easy
KMEANS_MIXED_FACTOR:50-50
CONFIDENCE_THRESHOLD:0.9
CONFIDENCE_MIN_CROPS:2500
CLIPPING_FACTOR:2.5
FEDPROX_FACTOR:1.0
FEDPROX_ADAPTIVE:False
USE_CLIPPING:True
USE_ENTROPY:True
USE_KMEANS:True
USE_FEDPROX:False
-----SETTINGS END-----
-----EXPERIMENT BEGINS-----
---------
Generating crops for FedDF
Namespace(img_size=32, batch_size=32, num_imgs=100000, threads=18, vflip=False, deg=30, shear=30, cropfirst=True, initcrop=0.5, scale=[500, 1], randinterp=False, debug=False, imgpath='./static/single_images/colonpath_sample.jpg', targetpath='/scratch-local/sunnys.3152562/1690367850_065061879', seed=127, img_per_thread=5555)
will save 100000 patches in /scratch-local/sunnys.3152562/1690367850_065061879/single_img_crops/crops
100000 took 0.63min with 18 threads
---------
Simulating feddf training
INFO simulate 2023-07-26 12:38:27,896 | simulate.py:50 | Python script started : simulate.py
DEBUG simulate 2023-07-26 12:38:27,896 | simulate.py:55 | Reading args from argument parser
Downloading https://zenodo.org/record/6496656/files/pathmnist.npz?download=1 to /scratch-local/sunnys.3152562/1690367850_065061879/pathmnist.npz
Generating unified pathmnist dataset
Class histogram for 0-th partition (alpha=10.0, 9 classes): [308 626 295 736 482 616 473 575 389]
INFO flwr 2023-07-26 12:40:56,004 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=30, round_timeout=None)
2023-07-26 12:40:59,109	INFO worker.py:1529 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
INFO flwr 2023-07-26 12:41:00,632 | app.py:179 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:145.136.62.9': 1.0, 'memory': 103079215104.0, 'accelerator_type:A100': 1.0, 'object_store_memory': 61847529062.0, 'CPU': 18.0}
INFO flwr 2023-07-26 12:41:00,633 | server.py:86 | Initializing global parameters
INFO flwr 2023-07-26 12:41:00,633 | server.py:266 | Using initial parameters provided by strategy
INFO flwr 2023-07-26 12:41:00,633 | server.py:88 | Evaluating initial parameters
INFO flwr 2023-07-26 12:41:09,589 | server.py:91 | initial parameters (loss, other metrics): 2.202056367815703, {'server_test_acc': 0.06044568245125348}
INFO flwr 2023-07-26 12:41:09,589 | server.py:101 | FL starting
Cluster score for round 1 = 813.5816040039062
INFO flwr 2023-07-26 12:41:17,301 | fed_df.py:230 | Number of selected crops : 10000
DEBUG flwr 2023-07-26 12:41:20,206 | server.py:215 | fit_round 1: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-07-26 12:41:45,368 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=116006, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 145, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 75, in train_model
    [1] == labels).sum().item()
RuntimeError: CUDA error: invalid program counter
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-07-26 12:41:46,044 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=116005, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 145, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 75, in train_model
    [1] == labels).sum().item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=116006)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=116005)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-07-26 12:41:49,108 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=115999, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 145, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 75, in train_model
    [1] == labels).sum().item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=115999)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-07-26 12:42:20,067 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=116004, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 145, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 75, in train_model
    [1] == labels).sum().item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=116004)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-07-26 12:42:35,594 | server.py:229 | fit_round 1 received 4 results and 4 failures
[2m[36m(launch_and_fit pid=116004)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=116005)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=115999)[0m Fitting Client 19
[2m[36m(launch_and_fit pid=116006)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=116003)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=116000)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=116001)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=116002)[0m Fitting Client 15
Warm start at round 1
Performing server side distillation training...
[2m[36m(launch_and_fit pid=116001)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=116000)[0m malloc_consolidate(): invalid chunk size
E0726 12:42:36.584829117  108067 chttp2_transport.cc:1079]             ipv4:145.136.62.9:56261: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 60000ms
[2m[36m(launch_and_fit pid=116003)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=116002)[0m double free or corruption (fasttop)
Distillation training stopped at step number : 500
Average fusion training loss : 0.37773419243097306, val accuracy : 0.42293241440640283, best val accuracy : 0.4799911071587372
Average local training loss : 8.752737603336573
Average local training accuracy : 0.6872422839506173
INFO flwr 2023-07-26 12:43:42,220 | server.py:116 | fit progress: (1, 1.945688883109345, {'server_test_acc': 0.34233983286908076}, 152.63032786897384)
INFO flwr 2023-07-26 12:43:42,220 | server.py:163 | evaluate_round 1: no clients selected, cancel
Cluster score for round 2 = 64088.6953125
INFO flwr 2023-07-26 12:43:50,968 | fed_df.py:230 | Number of selected crops : 6179
DEBUG flwr 2023-07-26 12:43:53,018 | server.py:215 | fit_round 2: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-07-26 12:44:45,676 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=351307, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 145, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 75, in train_model
    [1] == labels).sum().item()
RuntimeError: CUDA error: an illegal instruction was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-07-26 12:44:47,580 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=351305, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 145, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 75, in train_model
    [1] == labels).sum().item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=351307)[0m corrupted size vs. prev_size in fastbins
DEBUG flwr 2023-07-26 12:44:49,377 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=351236, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 145, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 75, in train_model
    [1] == labels).sum().item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=351305)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=351236)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=351235)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=351088)[0m corrupted size vs. prev_size in fastbins
DEBUG flwr 2023-07-26 12:45:34,359 | server.py:229 | fit_round 2 received 5 results and 3 failures
[2m[36m(launch_and_fit pid=351235)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=351306)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=351236)[0m Fitting Client 19
[2m[36m(launch_and_fit pid=351305)[0m Fitting Client 9
[2m[36m(launch_and_fit pid=351088)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=351125)[0m Fitting Client 4
[2m[36m(launch_and_fit pid=351237)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=351307)[0m Fitting Client 3
Warm start at round 2
Performing server side distillation training...
[2m[36m(launch_and_fit pid=351125)[0m corrupted size vs. prev_size in fastbins
[2m[36m(launch_and_fit pid=351306)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=351237)[0m double free or corruption (fasttop)
E0726 12:45:36.752695051  376328 chttp2_transport.cc:1079]             ipv4:145.136.62.9:56261: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 120000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.1019341929746554, best val accuracy : 0.1019341929746554
Average local training loss : nan
Average local training accuracy : 0.657753086419753
INFO flwr 2023-07-26 12:47:07,060 | server.py:116 | fit progress: (2, nan, {'server_test_acc': 0.18635097493036212}, 357.4705107189948)
INFO flwr 2023-07-26 12:47:07,060 | server.py:163 | evaluate_round 2: no clients selected, cancel
Cluster score for round 3 = inf
INFO flwr 2023-07-26 12:47:15,650 | fed_df.py:230 | Number of selected crops : 2500
Traceback (most recent call last):
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/./simulate.py", line 265, in <module>
    fl.simulation.start_simulation(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/app.py", line 196, in start_simulation
    hist = _fl(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/server/app.py", line 201, in _fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/server/server.py", line 106, in fit
    res_fit = self.fit_round(server_round=current_round, timeout=timeout)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/server/server.py", line 206, in fit_round
    client_instructions = self.strategy.configure_fit(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/strategy/fed_df.py", line 239, in configure_fit
    tsne_clusters = clustering.calculate_tsne(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/strategy/tools/clustering.py", line 195, in calculate_tsne
    tsne_embeddings = t_model.fit_transform(img_embeddings)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/cuml/internals/api_decorators.py", line 188, in wrapper
    ret = func(*args, **kwargs)
  File "t_sne.pyx", line 570, in cuml.manifold.t_sne.TSNE.fit_transform
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/cuml/internals/api_decorators.py", line 188, in wrapper
    ret = func(*args, **kwargs)
  File "t_sne.pyx", line 539, in cuml.manifold.t_sne.TSNE.fit
MemoryError: std::bad_alloc: CUDA error at: /home/sunnys/.conda/envs/single_image_fl/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
CUDA call='cudaEventDestroy(event_)' at file=/home/sunnys/.conda/envs/single_image_fl/include/raft/core/resource/cuda_event.hpp line=33 failed with an illegal memory access was encountered
double free or corruption (fasttop)
srun: error: gcn9: task 0: Aborted
srun: Terminating StepId=3152562.1
-----EXPERIMENT ENDS-----
-----CLEANING UP DISK------
-----END-----

JOB STATISTICS
==============
Job ID: 3152562
Cluster: snellius
User/Group: sunnys/sunnys
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:17:12
CPU Efficiency: 7.64% of 03:45:00 core-walltime
Job Wall-clock time: 00:12:30
Memory Utilized: 46.61 GB
Memory Efficiency: 38.84% of 120.00 GB
