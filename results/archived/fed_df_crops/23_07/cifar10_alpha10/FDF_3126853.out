-----BEGIN-----
-----SETTINGS-----
STRATEGY:feddf
MODEL_NAME:resnet8
NUM_CLIENTS:20
NUM_ROUNDS:30
FRACTION_FIT:0.4
FRACTION_EVALUATE:0.0
DATASET_NAME:cifar10
PARTITION_ALPHA:10.0
PARTITION_VAL_RATIO:0.1
BATCH_SIZE:512
LOCAL_EPOCHS:40
LOCAL_LR:0.05
DISTILL_BATCH_SIZE:512
SERVER_LR:0.005
SERVER_STEPS:500
SERVER_EARLY_STEPS:1000
USE_EARLY_STOPPING:False
USE_ADAPTIVE_LR:False
SEED:42
CUDA_DETERMINISTIC:False
USE_CROPS:True
IMG_NAME:ameyoko.jpg
DISTILL_DATASET:cifar100
DISTILL_ALPHA:1.0
NUM_DISTILL_IMAGES:20000
DISTILL_TRANSFORMS:v0
WARM_START:True
WARM_START_ROUNDS:1
WARM_START_INTERVAL:5
KMEANS_N_CLUSTER:20
KMEANS_HEURISTICS:easy
KMEANS_MIXED_FACTOR:50-50
CONFIDENCE_THRESHOLD:0.5
CLIPPING_FACTOR:2.5
-----SETTINGS END-----
-----EXPERIMENT BEGINS-----
---------
Generating crops for FedDF
Namespace(img_size=32, batch_size=32, num_imgs=100000, threads=18, vflip=False, deg=30, shear=30, cropfirst=True, initcrop=0.5, scale=[500, 1], randinterp=False, debug=False, imgpath='./static/single_images/ameyoko.jpg', targetpath='/scratch-local/sunnys.3126853/1690124929_313290956', seed=42, img_per_thread=5555)
will save 100000 patches in /scratch-local/sunnys.3126853/1690124929_313290956/single_img_crops/crops
100000 took 0.60min with 18 threads
---------
Simulating feddf training
INFO simulate 2023-07-23 17:09:42,368 | simulate.py:50 | Python script started : simulate.py
DEBUG simulate 2023-07-23 17:09:42,368 | simulate.py:55 | Reading args from argument parser
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /scratch-local/sunnys.3126853/1690124929_313290956/cifar-10-python.tar.gz
Extracting /scratch-local/sunnys.3126853/1690124929_313290956/cifar-10-python.tar.gz to /scratch-local/sunnys.3126853/1690124929_313290956
Generating unified cifar10 dataset
Class histogram for 0-th partition (alpha=10.0, 10 classes): [259 271 116 269 240 311 223 239 284 288]
INFO flwr 2023-07-23 17:12:19,887 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=30, round_timeout=None)
2023-07-23 17:12:22,546	INFO worker.py:1529 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
INFO flwr 2023-07-23 17:12:23,568 | app.py:179 | Flower VCE: Ray initialized with resources: {'node:145.136.62.9': 1.0, 'CPU': 18.0, 'object_store_memory': 61847529062.0, 'GPU': 1.0, 'accelerator_type:A100': 1.0, 'memory': 103079215104.0}
INFO flwr 2023-07-23 17:12:23,568 | server.py:86 | Initializing global parameters
INFO flwr 2023-07-23 17:12:23,568 | server.py:266 | Using initial parameters provided by strategy
INFO flwr 2023-07-23 17:12:23,568 | server.py:88 | Evaluating initial parameters
INFO flwr 2023-07-23 17:12:32,128 | server.py:91 | initial parameters (loss, other metrics): 2.3234217651367186, {'server_test_acc': 0.108}
INFO flwr 2023-07-23 17:12:32,129 | server.py:101 | FL starting
Cluster score for round 1 = 29085.03125
INFO flwr 2023-07-23 17:12:39,523 | fed_df.py:194 | Number of selected crops : 20000
DEBUG flwr 2023-07-23 17:12:44,049 | server.py:215 | fit_round 1: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-07-23 17:13:04,960 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2307476, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 118, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 54, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2307476)[0m double free or corruption (fasttop)
DEBUG flwr 2023-07-23 17:13:07,152 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2307442, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 118, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 54, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2307442)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-07-23 17:13:55,109 | server.py:229 | fit_round 1 received 6 results and 2 failures
[2m[36m(launch_and_fit pid=2307441)[0m malloc_consolidate(): invalid chunk size
[W] [17:12:39.733514] # of Nearest Neighbors should be at least 3 * perplexity. Your results might be a bit strange...
[2m[36m(launch_and_fit pid=2307441)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=2307463)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=2307464)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=2307442)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=2307474)[0m Fitting Client 5
[2m[36m(launch_and_fit pid=2307475)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=2307476)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=2307440)[0m Fitting Client 3
Warm start at round 1
Performing server side distillation training...
[2m[36m(launch_and_fit pid=2307440)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2307474)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2307464)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2307463)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2307475)[0m malloc_consolidate(): invalid chunk size
E0723 17:13:58.196510624 2313493 chttp2_transport.cc:1079]             ipv4:145.136.62.9:43077: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 60000ms
E0723 17:14:33.052150473 2378245 chttp2_transport.cc:1079]             ipv4:145.136.62.9:43077: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 120000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.1036, best val accuracy : 0.1036
Average local training loss : nan
Average local training accuracy : 0.2928592592592592
INFO flwr 2023-07-23 17:14:41,813 | server.py:116 | fit progress: (1, nan, {'server_test_acc': 0.1}, 129.6837926399894)
INFO flwr 2023-07-23 17:14:41,813 | server.py:163 | evaluate_round 1: no clients selected, cancel
Cluster score for round 2 = inf
INFO flwr 2023-07-23 17:14:49,952 | fed_df.py:194 | Number of selected crops : 3000
Traceback (most recent call last):
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/./simulate.py", line 256, in <module>
    fl.simulation.start_simulation(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/app.py", line 196, in start_simulation
    hist = _fl(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/server/app.py", line 201, in _fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/server/server.py", line 106, in fit
    res_fit = self.fit_round(server_round=current_round, timeout=timeout)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/server/server.py", line 206, in fit_round
    client_instructions = self.strategy.configure_fit(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/strategy/fed_df.py", line 203, in configure_fit
    tsne_clusters = clustering.calculate_tsne(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/strategy/tools/clustering.py", line 195, in calculate_tsne
    tsne_embeddings = t_model.fit_transform(img_embeddings)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/cuml/internals/api_decorators.py", line 188, in wrapper
    ret = func(*args, **kwargs)
  File "t_sne.pyx", line 570, in cuml.manifold.t_sne.TSNE.fit_transform
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/cuml/internals/api_decorators.py", line 188, in wrapper
    ret = func(*args, **kwargs)
  File "t_sne.pyx", line 539, in cuml.manifold.t_sne.TSNE.fit
MemoryError: std::bad_alloc: CUDA error at: /home/sunnys/.conda/envs/single_image_fl/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File "cupy_backends/cuda/api/driver.pyx", line 217, in cupy_backends.cuda.api.driver.moduleUnload
  File "cupy_backends/cuda/api/driver.pyx", line 60, in cupy_backends.cuda.api.driver.check_status
cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
CUDA call='cudaEventDestroy(event_)' at file=/home/sunnys/.conda/envs/single_image_fl/include/raft/core/resource/cuda_event.hpp line=33 failed with an illegal memory access was encountered
malloc_consolidate(): invalid chunk size
srun: error: gcn9: task 0: Aborted
srun: Terminating StepId=3126853.1
-----EXPERIMENT ENDS-----
-----CLEANING UP DISK------
-----END-----

JOB STATISTICS
==============
Job ID: 3126853
Cluster: snellius
User/Group: sunnys/sunnys
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:15:04
CPU Efficiency: 11.60% of 02:09:54 core-walltime
Job Wall-clock time: 00:07:13
Memory Utilized: 50.10 GB
Memory Efficiency: 41.75% of 120.00 GB
