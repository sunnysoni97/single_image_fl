-----BEGIN-----
-----SETTINGS-----
STRATEGY:feddf
MODEL_NAME:resnet8
NUM_CLIENTS:20
NUM_ROUNDS:30
FRACTION_FIT:0.4
FRACTION_EVALUATE:0.0
DATASET_NAME:cifar10
PARTITION_ALPHA:1.0
PARTITION_VAL_RATIO:0.1
BATCH_SIZE:1024
LOCAL_EPOCHS:80
LOCAL_LR:0.05
DISTILL_BATCH_SIZE:1024
SERVER_LR:0.005
SERVER_STEPS:500
SERVER_EARLY_STEPS:1000
USE_EARLY_STOPPING:False
USE_ADAPTIVE_LR:False
SEED:42
CUDA_DETERMINISTIC:False
USE_CROPS:True
IMG_NAME:ameyoko.jpg
DISTILL_DATASET:cifar100
DISTILL_ALPHA:1.0
NUM_DISTILL_IMAGES:2250
DISTILL_TRANSFORMS:v0
WARM_START:True
WARM_START_ROUNDS:1
WARM_START_INTERVAL:5
KMEANS_N_CLUSTER:100
KMEANS_HEURISTICS:easy
KMEANS_MIXED_FACTOR:50-50
-----SETTINGS END-----
-----EXPERIMENT BEGINS-----
---------
Generating crops for FedDF
Namespace(img_size=32, batch_size=32, num_imgs=100000, threads=12, vflip=False, deg=30, shear=30, cropfirst=True, initcrop=0.5, scale=[500, 1], randinterp=False, debug=False, imgpath='./static/single_images/ameyoko.jpg', targetpath='/scratch-local/sunnys.2914012/1686597213_277519558', seed=42, img_per_thread=8333)
will save 100000 patches in /scratch-local/sunnys.2914012/1686597213_277519558/single_img_crops/crops
100000 took 0.84min with 12 threads
---------
Simulating feddf training
INFO flwr 2023-06-12 21:18:11,270 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=30, round_timeout=None)
2023-06-12 21:18:14,537	INFO worker.py:1529 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8266 [39m[22m
INFO flwr 2023-06-12 21:18:18,240 | app.py:179 | Flower VCE: Ray initialized with resources: {'CPU': 72.0, 'memory': 265617823130.0, 'accelerator_type:A100': 1.0, 'node:145.136.62.9': 1.0, 'GPU': 1.0, 'object_store_memory': 118121924198.0}
INFO flwr 2023-06-12 21:18:18,240 | server.py:86 | Initializing global parameters
INFO flwr 2023-06-12 21:18:18,240 | server.py:266 | Using initial parameters provided by strategy
INFO flwr 2023-06-12 21:18:18,240 | server.py:88 | Evaluating initial parameters
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /scratch-local/sunnys.2914012/1686597213_277519558/cifar-10-python.tar.gz
Extracting /scratch-local/sunnys.2914012/1686597213_277519558/cifar-10-python.tar.gz to /scratch-local/sunnys.2914012/1686597213_277519558
Generating unified cifar10 dataset
Class histogram for 0-th partition (alpha=1.0, 10 classes): [440 381 415  39  10 257 227 532  17 182]
INFO flwr 2023-06-12 21:18:23,852 | server.py:91 | initial parameters (loss, other metrics): 2.323421728515625, {'server_test_acc': 0.108}
INFO flwr 2023-06-12 21:18:23,852 | server.py:101 | FL starting
DEBUG flwr 2023-06-12 21:18:31,645 | server.py:215 | fit_round 1: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 21:18:52,154 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1505368, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 21:18:53,431 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1505373, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1505368)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1505373)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:19:23,920 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1505371, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1505371)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1505372)[0m corrupted size vs. prev_size in fastbins
[2m[36m(launch_and_fit pid=1505374)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1505369)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1505367)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:20:39,785 | server.py:229 | fit_round 1 received 5 results and 3 failures
/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/server/strategy/aggregate.py:33: RuntimeWarning: invalid value encountered in multiply
  [layer * num_examples for layer in weights] for weights, num_examples in results
Cluster score for round 1 = 15921.025390625
[2m[36m(launch_and_fit pid=1505370)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=1505368)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=1505367)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=1505369)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=1505371)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=1505372)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=1505373)[0m Fitting Client 5
[2m[36m(launch_and_fit pid=1505374)[0m Fitting Client 13
Warm start at round 1
Performing server side distillation training...
[2m[36m(launch_and_fit pid=1505370)[0m malloc_consolidate(): invalid chunk size
E0612 21:20:44.920653742 1802875 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 60000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.3937399999999999
INFO flwr 2023-06-12 21:22:03,157 | server.py:116 | fit progress: (1, nan, {'server_test_acc': 0.1}, 219.30515015823767)
INFO flwr 2023-06-12 21:22:03,157 | server.py:163 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2023-06-12 21:22:11,457 | server.py:215 | fit_round 2: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 21:22:30,595 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1905298, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 21:22:30,780 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1905304, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1905298)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=1905304)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1905308)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1905293)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1905300)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:23:58,126 | server.py:229 | fit_round 2 received 6 results and 2 failures
Cluster score for round 2 = inf
[2m[36m(launch_and_fit pid=1905293)[0m Fitting Client 5
[2m[36m(launch_and_fit pid=1905298)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=1905304)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=1905299)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=1905296)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=1905303)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=1905300)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=1905308)[0m Fitting Client 13
Performing server side distillation training...
[2m[36m(launch_and_fit pid=1905299)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1905303)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1905296)[0m malloc_consolidate(): invalid chunk size
E0612 21:24:00.576620890 2190344 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 120000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.10222222222222221
INFO flwr 2023-06-12 21:26:29,890 | server.py:116 | fit progress: (2, nan, {'server_test_acc': 0.1}, 486.03860970493406)
INFO flwr 2023-06-12 21:26:29,891 | server.py:163 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2023-06-12 21:26:38,243 | server.py:215 | fit_round 3: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 21:26:54,126 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2384562, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2384562)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:26:57,960 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2384568, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2384568)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:27:03,006 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2384573, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2384573)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:28:14,085 | server.py:229 | fit_round 3 received 5 results and 3 failures
Cluster score for round 3 = inf
[2m[36m(launch_and_fit pid=2384572)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=2384562)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=2384576)[0m Fitting Client 4
[2m[36m(launch_and_fit pid=2384567)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=2384568)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=2384573)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=2384569)[0m Fitting Client 5
[2m[36m(launch_and_fit pid=2384565)[0m Fitting Client 17
Performing server side distillation training...
[2m[36m(launch_and_fit pid=2384572)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2384576)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2384569)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2384567)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2384565)[0m malloc_consolidate(): invalid chunk size
E0612 21:28:26.903768940 2448781 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 240000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.09635555555555557
INFO flwr 2023-06-12 21:30:32,737 | server.py:116 | fit progress: (3, nan, {'server_test_acc': 0.1}, 728.8853541640565)
INFO flwr 2023-06-12 21:30:32,737 | server.py:163 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2023-06-12 21:30:41,149 | server.py:215 | fit_round 4: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 21:30:57,464 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2844637, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2844637)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:31:09,519 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2844499, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2844499)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:31:54,432 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2844601, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2844601)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:32:22,084 | server.py:229 | fit_round 4 received 5 results and 3 failures
Cluster score for round 4 = inf
[2m[36m(launch_and_fit pid=2844498)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=2844499)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=2844600)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=2844497)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=2844399)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=2844636)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=2844637)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2844601)[0m Fitting Client 12
Performing server side distillation training...
[2m[36m(launch_and_fit pid=2844600)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2844498)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2844636)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2844497)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2844399)[0m malloc_consolidate(): invalid chunk size
E0612 21:32:32.090290105 3054972 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 480000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.08293333333333333
INFO flwr 2023-06-12 21:34:43,118 | server.py:116 | fit progress: (4, nan, {'server_test_acc': 0.1}, 979.2658520471305)
INFO flwr 2023-06-12 21:34:43,118 | server.py:163 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2023-06-12 21:34:51,291 | server.py:215 | fit_round 5: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 21:35:09,533 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3313222, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3313222)[0m corrupted size vs. prev_size in fastbins
DEBUG flwr 2023-06-12 21:35:16,513 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3313226, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3313226)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:35:19,687 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3313224, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3313224)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:35:55,741 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3313223, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3313223)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=3313219)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:36:23,899 | server.py:229 | fit_round 5 received 4 results and 4 failures
Cluster score for round 5 = inf
[2m[36m(launch_and_fit pid=3313225)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=3313221)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=3313219)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=3313216)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=3313223)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=3313224)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=3313226)[0m Fitting Client 9
[2m[36m(launch_and_fit pid=3313222)[0m Fitting Client 2
Warm start at round 5
Performing server side distillation training...
[2m[36m(launch_and_fit pid=3313221)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3313216)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3313225)[0m double free or corruption (fasttop)
E0612 21:38:01.790076857 3488575 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 960000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.12877777777777777
INFO flwr 2023-06-12 21:38:43,869 | server.py:116 | fit progress: (5, nan, {'server_test_acc': 0.1}, 1220.0173151763156)
INFO flwr 2023-06-12 21:38:43,869 | server.py:163 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2023-06-12 21:38:52,025 | server.py:215 | fit_round 6: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 21:39:18,976 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3752047, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3752047)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:39:23,891 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3751954, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: invalid program counter
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3751954)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:39:29,106 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3751952, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3751952)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:40:30,780 | server.py:229 | fit_round 6 received 5 results and 3 failures
Cluster score for round 6 = inf
[2m[36m(launch_and_fit pid=3751954)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=3752001)[0m Fitting Client 4
[2m[36m(launch_and_fit pid=3751953)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=3752047)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=3751975)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=3751952)[0m Fitting Client 15
[2m[36m(launch_and_fit pid=3752125)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=3751977)[0m Fitting Client 12
Performing server side distillation training...
[2m[36m(launch_and_fit pid=3752125)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3751977)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3752001)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=3751975)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=3751953)[0m corrupted size vs. prev_size in fastbins
E0612 21:41:14.980072538 3848815 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 1920000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.12026666666666667
INFO flwr 2023-06-12 21:42:57,343 | server.py:116 | fit progress: (6, nan, {'server_test_acc': 0.1}, 1473.4908746089786)
INFO flwr 2023-06-12 21:42:57,343 | server.py:163 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2023-06-12 21:43:05,720 | server.py:215 | fit_round 7: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 21:43:22,065 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=72616, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=72616)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:43:33,727 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=72627, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=72627)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 21:43:47,468 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=72612, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=72612)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=72624)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 21:44:42,787 | server.py:229 | fit_round 7 received 5 results and 3 failures
Cluster score for round 7 = inf
[2m[36m(launch_and_fit pid=72627)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=72624)[0m Fitting Client 9
[2m[36m(launch_and_fit pid=72612)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=72616)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=72614)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=72619)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=72610)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=72626)[0m Fitting Client 6
Performing server side distillation training...
[2m[36m(launch_and_fit pid=72614)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=72619)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=72626)[0m corrupted size vs. prev_size in fastbins
[2m[36m(launch_and_fit pid=72610)[0m double free or corruption (fasttop)
E0612 21:44:49.222983984  297197 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 3840000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.07786666666666667
INFO flwr 2023-06-12 21:47:06,205 | server.py:116 | fit progress: (7, nan, {'server_test_acc': 0.1}, 1722.3527083951049)
INFO flwr 2023-06-12 21:47:06,205 | server.py:163 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2023-06-12 21:47:14,424 | server.py:215 | fit_round 8: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 21:47:32,097 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=534219, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=534219)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:47:39,486 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=534207, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=534207)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:47:46,681 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=534209, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=534209)[0m corrupted size vs. prev_size in fastbins
DEBUG flwr 2023-06-12 21:48:51,121 | server.py:229 | fit_round 8 received 5 results and 3 failures
Cluster score for round 8 = inf
[2m[36m(launch_and_fit pid=534211)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=534213)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=534220)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=534212)[0m Fitting Client 15
[2m[36m(launch_and_fit pid=534219)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=534207)[0m Fitting Client 4
[2m[36m(launch_and_fit pid=534222)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=534209)[0m Fitting Client 7
Performing server side distillation training...
[2m[36m(launch_and_fit pid=534222)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=534212)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=534213)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=534220)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=534211)[0m double free or corruption (fasttop)
E0612 21:49:25.706772231  759827 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 7680000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.10675555555555556
INFO flwr 2023-06-12 21:51:14,204 | server.py:116 | fit progress: (8, nan, {'server_test_acc': 0.1}, 1970.3522787331603)
INFO flwr 2023-06-12 21:51:14,204 | server.py:163 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2023-06-12 21:51:22,613 | server.py:215 | fit_round 9: strategy sampled 8 clients (out of 20)
[2m[36m(launch_and_fit pid=994886)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=994882)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=995037)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=995030)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:53:29,539 | server.py:229 | fit_round 9 received 8 results and 0 failures
Cluster score for round 9 = inf
[2m[36m(launch_and_fit pid=994919)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=995037)[0m Fitting Client 9
[2m[36m(launch_and_fit pid=994879)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=995030)[0m Fitting Client 4
[2m[36m(launch_and_fit pid=994882)[0m Fitting Client 5
[2m[36m(launch_and_fit pid=994878)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=994886)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=995036)[0m Fitting Client 14
Performing server side distillation training...
[2m[36m(launch_and_fit pid=994919)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=994879)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=994878)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=995036)[0m malloc_consolidate(): invalid chunk size
E0612 21:53:35.055555587 1244638 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 15360000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.09627777777777778
INFO flwr 2023-06-12 21:55:52,534 | server.py:116 | fit progress: (9, nan, {'server_test_acc': 0.1}, 2248.682548648212)
INFO flwr 2023-06-12 21:55:52,535 | server.py:163 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2023-06-12 21:56:00,804 | server.py:215 | fit_round 10: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 21:56:32,892 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1521758, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: invalid program counter
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1521758)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 21:56:46,599 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1521754, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1521754)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1521761)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 21:57:49,045 | server.py:229 | fit_round 10 received 6 results and 2 failures
Cluster score for round 10 = inf
[2m[36m(launch_and_fit pid=1521754)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=1521755)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=1521758)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1521689)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=1521760)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=1521762)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=1521757)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=1521761)[0m Fitting Client 17
Warm start at round 10
Performing server side distillation training...
[2m[36m(launch_and_fit pid=1521762)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1521760)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=1521689)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1521755)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1521757)[0m malloc_consolidate(): invalid chunk size
E0612 21:57:57.530224256 1783492 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 30720000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.09422222222222225
INFO flwr 2023-06-12 22:00:15,532 | server.py:116 | fit progress: (10, nan, {'server_test_acc': 0.1}, 2511.6801078082062)
INFO flwr 2023-06-12 22:00:15,532 | server.py:163 | evaluate_round 10: no clients selected, cancel
DEBUG flwr 2023-06-12 22:00:23,809 | server.py:215 | fit_round 11: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:00:39,451 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2038494, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 48, in train_model
    loss.backward()
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: an illegal instruction was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:00:39,793 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2038492, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2038494)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2038492)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2038399)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2038493)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:02:08,494 | server.py:229 | fit_round 11 received 6 results and 2 failures
Cluster score for round 11 = inf
[2m[36m(launch_and_fit pid=2038494)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=2038491)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=2038493)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=2038492)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=2038490)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=2038489)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=2038468)[0m Fitting Client 9
[2m[36m(launch_and_fit pid=2038399)[0m Fitting Client 8
Performing server side distillation training...
[2m[36m(launch_and_fit pid=2038468)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2038489)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2038491)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2038490)[0m malloc_consolidate(): invalid chunk size
E0612 22:02:17.718118554 2294869 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 61440000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.1114074074074074
INFO flwr 2023-06-12 22:04:30,976 | server.py:116 | fit progress: (11, nan, {'server_test_acc': 0.1}, 2767.1243827301078)
INFO flwr 2023-06-12 22:04:30,976 | server.py:163 | evaluate_round 11: no clients selected, cancel
DEBUG flwr 2023-06-12 22:04:39,377 | server.py:215 | fit_round 12: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:05:01,735 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2498582, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2498582)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:05:06,857 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2498588, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:05:08,067 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2498589, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2498588)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2498589)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2498584)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2498585)[0m corrupted size vs. prev_size in fastbins
DEBUG flwr 2023-06-12 22:06:20,446 | server.py:229 | fit_round 12 received 5 results and 3 failures
Cluster score for round 12 = inf
[2m[36m(launch_and_fit pid=2498586)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=2498588)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=2498589)[0m Fitting Client 4
[2m[36m(launch_and_fit pid=2498584)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=2498587)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=2498582)[0m Fitting Client 5
[2m[36m(launch_and_fit pid=2498579)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=2498585)[0m Fitting Client 16
Performing server side distillation training...
[2m[36m(launch_and_fit pid=2498579)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2498587)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2498586)[0m malloc_consolidate(): invalid chunk size
E0612 22:06:23.511658876 2748857 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 122880000ms
E0612 22:06:45.894268064 2751626 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 245760000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.12053333333333334
INFO flwr 2023-06-12 22:08:43,522 | server.py:116 | fit progress: (12, nan, {'server_test_acc': 0.1}, 3019.6700550261885)
INFO flwr 2023-06-12 22:08:43,522 | server.py:163 | evaluate_round 12: no clients selected, cancel
DEBUG flwr 2023-06-12 22:08:51,677 | server.py:215 | fit_round 13: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:09:13,339 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2930364, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:09:13,739 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2930355, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2930364)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2930355)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2930363)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 22:10:38,484 | server.py:229 | fit_round 13 received 6 results and 2 failures
Cluster score for round 13 = inf
[2m[36m(launch_and_fit pid=2930370)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=2930360)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=2930355)[0m Fitting Client 4
[2m[36m(launch_and_fit pid=2930363)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=2930364)[0m Fitting Client 5
[2m[36m(launch_and_fit pid=2930352)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=2930369)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=2930371)[0m Fitting Client 14
Performing server side distillation training...
[2m[36m(launch_and_fit pid=2930370)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2930360)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2930369)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=2930371)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=2930352)[0m malloc_consolidate(): invalid chunk size
E0612 22:10:51.609939085 3254458 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 491520000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.1188148148148148
INFO flwr 2023-06-12 22:13:00,487 | server.py:116 | fit progress: (13, nan, {'server_test_acc': 0.1}, 3276.6349255461246)
INFO flwr 2023-06-12 22:13:00,487 | server.py:163 | evaluate_round 13: no clients selected, cancel
DEBUG flwr 2023-06-12 22:13:08,869 | server.py:215 | fit_round 14: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:13:23,980 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3436604, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:13:25,518 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3436619, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3436604)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3436619)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:13:28,698 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3436615, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3436615)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3436636)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 22:14:44,266 | server.py:229 | fit_round 14 received 5 results and 3 failures
Cluster score for round 14 = inf
[2m[36m(launch_and_fit pid=3436607)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=3436604)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=3436636)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=3436627)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=3436623)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=3436624)[0m Fitting Client 5
[2m[36m(launch_and_fit pid=3436615)[0m Fitting Client 19
[2m[36m(launch_and_fit pid=3436619)[0m Fitting Client 15
Performing server side distillation training...
[2m[36m(launch_and_fit pid=3436607)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3436623)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3436624)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3436627)[0m malloc_consolidate(): invalid chunk size
E0612 22:14:53.528970056 3721036 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 983040000ms
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.10222222222222224
INFO flwr 2023-06-12 22:17:07,081 | server.py:116 | fit progress: (14, nan, {'server_test_acc': 0.1}, 3523.2295116651803)
INFO flwr 2023-06-12 22:17:07,082 | server.py:163 | evaluate_round 14: no clients selected, cancel
DEBUG flwr 2023-06-12 22:17:15,336 | server.py:215 | fit_round 15: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:17:30,665 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3902939, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:17:31,100 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3902926, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3902939)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:17:32,881 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3902934, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal instruction was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3902926)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=3902934)[0m corrupted size vs. prev_size in fastbins
[2m[36m(launch_and_fit pid=3902933)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3902941)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=3902942)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:18:49,452 | server.py:229 | fit_round 15 received 5 results and 3 failures
Cluster score for round 15 = inf
[2m[36m(launch_and_fit pid=3902933)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3902939)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=3902943)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=3902926)[0m Fitting Client 9
[2m[36m(launch_and_fit pid=3902934)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=3902941)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=3902942)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=3902928)[0m Fitting Client 7
Warm start at round 15
Performing server side distillation training...
[2m[36m(launch_and_fit pid=3902943)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3902928)[0m malloc_consolidate(): invalid chunk size
E0612 22:19:03.743431319 3939247 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): 1966080000ms
E0612 22:20:39.069156024 4162172 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.03671111111111111
INFO flwr 2023-06-12 22:21:11,363 | server.py:116 | fit progress: (15, nan, {'server_test_acc': 0.1}, 3767.5109279579483)
INFO flwr 2023-06-12 22:21:11,363 | server.py:163 | evaluate_round 15: no clients selected, cancel
DEBUG flwr 2023-06-12 22:21:19,816 | server.py:215 | fit_round 16: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:21:36,328 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=158402, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:21:36,652 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=158535, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:21:38,208 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=158397, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=158402)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=158535)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=158397)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:21:41,763 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=158536, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=158536)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:21:51,245 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=158403, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal instruction was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=158403)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:22:38,047 | server.py:229 | fit_round 16 received 3 results and 5 failures
Cluster score for round 16 = inf
[2m[36m(launch_and_fit pid=158404)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=158397)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=158398)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=158535)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=158399)[0m Fitting Client 4
[2m[36m(launch_and_fit pid=158536)[0m Fitting Client 19
[2m[36m(launch_and_fit pid=158402)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=158403)[0m Fitting Client 15
Performing server side distillation training...
[2m[36m(launch_and_fit pid=158398)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=158399)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=158404)[0m malloc_consolidate(): invalid chunk size
E0612 22:22:40.447257870  233286 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
E0612 22:23:39.247772222  344542 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.15466666666666665
INFO flwr 2023-06-12 22:25:01,733 | server.py:116 | fit progress: (16, nan, {'server_test_acc': 0.1}, 3997.881610073149)
INFO flwr 2023-06-12 22:25:01,734 | server.py:163 | evaluate_round 16: no clients selected, cancel
DEBUG flwr 2023-06-12 22:25:09,940 | server.py:215 | fit_round 17: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:25:26,038 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=546911, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:25:27,027 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=546804, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=546911)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=546804)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:25:37,179 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=546805, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=546805)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:25:59,590 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=546925, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=546925)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:26:40,186 | server.py:229 | fit_round 17 received 4 results and 4 failures
Cluster score for round 17 = inf
[2m[36m(launch_and_fit pid=546805)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=546909)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=546845)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=546804)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=546910)[0m Fitting Client 9
[2m[36m(launch_and_fit pid=546925)[0m Fitting Client 19
[2m[36m(launch_and_fit pid=546911)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=546926)[0m Fitting Client 18
Performing server side distillation training...
[2m[36m(launch_and_fit pid=546909)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=546910)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=546845)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=546926)[0m malloc_consolidate(): invalid chunk size
E0612 22:26:44.891170290  766489 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.08166666666666667
INFO flwr 2023-06-12 22:29:03,091 | server.py:116 | fit progress: (17, nan, {'server_test_acc': 0.1}, 4239.238651681226)
INFO flwr 2023-06-12 22:29:03,091 | server.py:163 | evaluate_round 17: no clients selected, cancel
DEBUG flwr 2023-06-12 22:29:11,469 | server.py:215 | fit_round 18: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:29:30,242 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1014801, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:29:30,319 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1014942, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1014801)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1014942)[0m corrupted size vs. prev_size in fastbins
DEBUG flwr 2023-06-12 22:29:32,641 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1014950, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal instruction was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1014950)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:29:37,704 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1014840, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1014840)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:30:37,890 | server.py:229 | fit_round 18 received 4 results and 4 failures
Cluster score for round 18 = inf
[2m[36m(launch_and_fit pid=1014950)[0m Fitting Client 15
[2m[36m(launch_and_fit pid=1014952)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1014802)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=1014942)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=1014840)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=1014801)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=1014841)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=1014951)[0m Fitting Client 17
Performing server side distillation training...
[2m[36m(launch_and_fit pid=1014841)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1014802)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=1014952)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1014951)[0m malloc_consolidate(): invalid chunk size
E0612 22:32:30.497882143 1341671 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.09633333333333335
INFO flwr 2023-06-12 22:33:02,202 | server.py:116 | fit progress: (18, nan, {'server_test_acc': 0.1}, 4478.350068741012)
INFO flwr 2023-06-12 22:33:02,202 | server.py:163 | evaluate_round 18: no clients selected, cancel
DEBUG flwr 2023-06-12 22:33:10,486 | server.py:215 | fit_round 19: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:33:26,220 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1425189, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1425189)[0m corrupted size vs. prev_size in fastbins
DEBUG flwr 2023-06-12 22:33:41,940 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1425348, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1425348)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 22:33:49,475 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1425347, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal instruction was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1425347)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:34:46,273 | server.py:229 | fit_round 19 received 5 results and 3 failures
Cluster score for round 19 = inf
[2m[36m(launch_and_fit pid=1425189)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=1425230)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=1425348)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=1425350)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=1425195)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1425349)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1425347)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=1425208)[0m Fitting Client 17
Performing server side distillation training...
[2m[36m(launch_and_fit pid=1425230)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=1425349)[0m malloc_consolidate(): invalid chunk size
E0612 22:34:47.264940851 1660015 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
[2m[36m(launch_and_fit pid=1425350)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1425208)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1425195)[0m malloc_consolidate(): invalid chunk size
E0612 22:35:35.698804134 1660260 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.0863111111111111
INFO flwr 2023-06-12 22:37:09,794 | server.py:116 | fit progress: (19, nan, {'server_test_acc': 0.1}, 4725.9419109351)
INFO flwr 2023-06-12 22:37:09,794 | server.py:163 | evaluate_round 19: no clients selected, cancel
DEBUG flwr 2023-06-12 22:37:18,333 | server.py:215 | fit_round 20: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:37:35,926 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1893889, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1893889)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:37:39,007 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1894012, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1894012)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 22:37:43,649 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1894014, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1894014)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1893891)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=1894011)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 22:38:54,495 | server.py:229 | fit_round 20 received 5 results and 3 failures
Cluster score for round 20 = inf
[2m[36m(launch_and_fit pid=1893873)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=1893891)[0m Fitting Client 10
[2m[36m(launch_and_fit pid=1893819)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=1894012)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=1893886)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=1893889)[0m Fitting Client 19
[2m[36m(launch_and_fit pid=1894014)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=1894011)[0m Fitting Client 16
Warm start at round 20
Performing server side distillation training...
[2m[36m(launch_and_fit pid=1893819)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1893873)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1893886)[0m malloc_consolidate(): invalid chunk size
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.07324444444444445
INFO flwr 2023-06-12 22:41:18,912 | server.py:116 | fit progress: (20, nan, {'server_test_acc': 0.1}, 4975.0604462693445)
INFO flwr 2023-06-12 22:41:18,913 | server.py:163 | evaluate_round 20: no clients selected, cancel
DEBUG flwr 2023-06-12 22:41:27,130 | server.py:215 | fit_round 21: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:41:43,292 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2349249, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:41:44,333 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2349250, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2349249)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2349250)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:42:36,749 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2349383, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2349383)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 22:43:06,981 | server.py:229 | fit_round 21 received 5 results and 3 failures
Cluster score for round 21 = inf
[2m[36m(launch_and_fit pid=2349249)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=2349252)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=2349250)[0m Fitting Client 4
[2m[36m(launch_and_fit pid=2349255)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=2349355)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=2349390)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=2349383)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=2349251)[0m Fitting Client 15
Performing server side distillation training...
[2m[36m(launch_and_fit pid=2349255)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2349355)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2349251)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2349252)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2349390)[0m malloc_consolidate(): invalid chunk size
E0612 22:43:34.725906127 2479127 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.11991111111111112
INFO flwr 2023-06-12 22:45:36,123 | server.py:116 | fit progress: (21, nan, {'server_test_acc': 0.1}, 5232.271084469277)
INFO flwr 2023-06-12 22:45:36,123 | server.py:163 | evaluate_round 21: no clients selected, cancel
DEBUG flwr 2023-06-12 22:45:44,547 | server.py:215 | fit_round 22: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:45:58,742 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2850356, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2850356)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:46:05,309 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2850403, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2850403)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 22:46:11,035 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=2850354, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=2850354)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2850357)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:47:18,785 | server.py:229 | fit_round 22 received 5 results and 3 failures
Cluster score for round 22 = inf
[2m[36m(launch_and_fit pid=2850356)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=2850351)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=2850355)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=2850357)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=2850353)[0m Fitting Client 19
[2m[36m(launch_and_fit pid=2850354)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=2850403)[0m Fitting Client 15
[2m[36m(launch_and_fit pid=2850360)[0m Fitting Client 5
Performing server side distillation training...
[2m[36m(launch_and_fit pid=2850353)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2850351)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2850355)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=2850360)[0m double free or corruption (fasttop)
E0612 22:47:24.078736041 2886266 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.06968888888888888
INFO flwr 2023-06-12 22:49:47,901 | server.py:116 | fit progress: (22, nan, {'server_test_acc': 0.1}, 5484.04941098718)
INFO flwr 2023-06-12 22:49:47,902 | server.py:163 | evaluate_round 22: no clients selected, cancel
DEBUG flwr 2023-06-12 22:49:56,339 | server.py:215 | fit_round 23: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:50:41,645 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3288878, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3288878)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:50:43,684 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3288882, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:50:44,029 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3288867, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal instruction was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:50:44,375 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3288866, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3288866)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=3288882)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3288867)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=3288877)[0m double free or corruption (fasttop)
DEBUG flwr 2023-06-12 22:51:51,301 | server.py:229 | fit_round 23 received 4 results and 4 failures
Cluster score for round 23 = inf
[2m[36m(launch_and_fit pid=3288867)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=3288880)[0m Fitting Client 11
[2m[36m(launch_and_fit pid=3288882)[0m Fitting Client 15
[2m[36m(launch_and_fit pid=3288862)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=3288877)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=3288863)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=3288866)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=3288878)[0m Fitting Client 5
Performing server side distillation training...
[2m[36m(launch_and_fit pid=3288862)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3288863)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3288880)[0m malloc_consolidate(): invalid chunk size
E0612 22:51:55.587893890 3460239 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
E0612 22:52:22.527002588 1878288 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.101
INFO flwr 2023-06-12 22:54:06,415 | server.py:116 | fit progress: (23, nan, {'server_test_acc': 0.1}, 5742.562806954142)
INFO flwr 2023-06-12 22:54:06,415 | server.py:163 | evaluate_round 23: no clients selected, cancel
DEBUG flwr 2023-06-12 22:54:14,230 | server.py:215 | fit_round 24: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:54:29,049 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3700322, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 22:54:29,610 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3700323, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3700322)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=3700323)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:54:37,858 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=3700315, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=3700315)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3700314)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:55:45,512 | server.py:229 | fit_round 24 received 5 results and 3 failures
Cluster score for round 24 = inf
[2m[36m(launch_and_fit pid=3700320)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=3700322)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=3700319)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=3700315)[0m Fitting Client 6
[2m[36m(launch_and_fit pid=3700323)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=3700324)[0m Fitting Client 15
[2m[36m(launch_and_fit pid=3700321)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=3700314)[0m Fitting Client 3
Performing server side distillation training...
[2m[36m(launch_and_fit pid=3700324)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3700320)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3700319)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=3700321)[0m corrupted size vs. prev_size in fastbins
E0612 22:57:48.114539065 4102307 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.06888888888888889
INFO flwr 2023-06-12 22:57:59,806 | server.py:116 | fit progress: (24, nan, {'server_test_acc': 0.1}, 5975.953818172216)
INFO flwr 2023-06-12 22:57:59,806 | server.py:163 | evaluate_round 24: no clients selected, cancel
DEBUG flwr 2023-06-12 22:58:07,647 | server.py:215 | fit_round 25: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 22:58:55,009 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=4120191, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=4120191)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:58:57,897 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=4120118, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=4120118)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 22:59:02,443 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=4120189, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=4120189)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=4120192)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=4120194)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 23:00:16,841 | server.py:229 | fit_round 25 received 5 results and 3 failures
Cluster score for round 25 = inf
[2m[36m(launch_and_fit pid=4120118)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=4120191)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=4120192)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=4120193)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=4120190)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=4120189)[0m Fitting Client 7
[2m[36m(launch_and_fit pid=4120188)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=4120194)[0m Fitting Client 19
Warm start at round 25
Performing server side distillation training...
[2m[36m(launch_and_fit pid=4120188)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=4120193)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=4120190)[0m malloc_consolidate(): invalid chunk size
E0612 23:00:41.900806199  245138 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.13315555555555555
INFO flwr 2023-06-12 23:02:29,035 | server.py:116 | fit progress: (25, nan, {'server_test_acc': 0.1}, 6245.182967764325)
INFO flwr 2023-06-12 23:02:29,035 | server.py:163 | evaluate_round 25: no clients selected, cancel
DEBUG flwr 2023-06-12 23:02:36,859 | server.py:215 | fit_round 26: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 23:02:51,853 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=403367, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=403367)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 23:03:01,044 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=403451, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal instruction was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=403451)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 23:03:05,862 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=403452, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=403452)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 23:03:14,716 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=403449, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=403449)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=403445)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=403450)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 23:04:05,380 | server.py:229 | fit_round 26 received 4 results and 4 failures
Cluster score for round 26 = inf
[2m[36m(launch_and_fit pid=403444)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=403449)[0m Fitting Client 2
[2m[36m(launch_and_fit pid=403432)[0m Fitting Client 1
[2m[36m(launch_and_fit pid=403367)[0m Fitting Client 3
[2m[36m(launch_and_fit pid=403450)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=403445)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=403451)[0m Fitting Client 14
[2m[36m(launch_and_fit pid=403452)[0m Fitting Client 19
Performing server side distillation training...
[2m[36m(launch_and_fit pid=403432)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=403444)[0m malloc_consolidate(): invalid chunk size
E0612 23:05:39.990196401  457974 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.10155555555555555
INFO flwr 2023-06-12 23:06:17,670 | server.py:116 | fit progress: (26, nan, {'server_test_acc': 0.1}, 6473.818376403302)
INFO flwr 2023-06-12 23:06:17,670 | server.py:163 | evaluate_round 26: no clients selected, cancel
DEBUG flwr 2023-06-12 23:06:25,514 | server.py:215 | fit_round 27: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 23:06:43,436 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=807143, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 23:06:44,556 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=807139, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=807143)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=807139)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 23:06:48,222 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=807141, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=807141)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=807144)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 23:08:05,276 | server.py:229 | fit_round 27 received 5 results and 3 failures
Cluster score for round 27 = inf
[2m[36m(launch_and_fit pid=807136)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=807142)[0m Fitting Client 5
[2m[36m(launch_and_fit pid=807143)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=807138)[0m Fitting Client 17
[2m[36m(launch_and_fit pid=807140)[0m Fitting Client 15
[2m[36m(launch_and_fit pid=807141)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=807144)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=807139)[0m Fitting Client 6
Performing server side distillation training...
[2m[36m(launch_and_fit pid=807138)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=807140)[0m double free or corruption (fasttop)
[2m[36m(launch_and_fit pid=807136)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=807142)[0m malloc_consolidate(): invalid chunk size
Distillation training stopped at step number : 500
Average fusion training loss : nan, val accuracy : 0.09240000000000001, best val accuracy : 0.0924
Average local training loss : nan
Average local training accuracy : 0.09368888888888888
INFO flwr 2023-06-12 23:10:22,819 | server.py:116 | fit progress: (27, nan, {'server_test_acc': 0.1}, 6718.9670773851685)
INFO flwr 2023-06-12 23:10:22,819 | server.py:163 | evaluate_round 27: no clients selected, cancel
DEBUG flwr 2023-06-12 23:10:30,662 | server.py:215 | fit_round 28: strategy sampled 8 clients (out of 20)
DEBUG flwr 2023-06-12 23:10:47,003 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1228505, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
DEBUG flwr 2023-06-12 23:10:47,200 | ray_client_proxy.py:87 | [36mray::launch_and_fit()[39m (pid=1228435, ip=145.136.62.9)
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 148, in launch_and_fit
    return maybe_call_fit(
  File "/home/sunnys/.conda/envs/single_image_fl/lib/python3.10/site-packages/flwr/client/client.py", line 184, in maybe_call_fit
    return client.fit(fit_ins)
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 116, in fit
    new_parameters, distill_preds, train_res = train_model(
  File "/gpfs/home1/sunnys/thesis_23/single_image_fl/client/fed_df.py", line 52, in train_model
    epoch_loss += loss.item()
RuntimeError: CUDA error: an illegal instruction was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
[2m[36m(launch_and_fit pid=1228435)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1228505)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1228430)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1228432)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1228434)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1228437)[0m malloc_consolidate(): invalid chunk size
DEBUG flwr 2023-06-12 23:12:19,730 | server.py:229 | fit_round 28 received 6 results and 2 failures
Cluster score for round 28 = inf
[2m[36m(launch_and_fit pid=1228435)[0m Fitting Client 8
[2m[36m(launch_and_fit pid=1228434)[0m Fitting Client 18
[2m[36m(launch_and_fit pid=1228437)[0m Fitting Client 16
[2m[36m(launch_and_fit pid=1228432)[0m Fitting Client 12
[2m[36m(launch_and_fit pid=1228505)[0m Fitting Client 13
[2m[36m(launch_and_fit pid=1228430)[0m Fitting Client 5
[2m[36m(launch_and_fit pid=1228433)[0m Fitting Client 0
[2m[36m(launch_and_fit pid=1228436)[0m Fitting Client 14
Performing server side distillation training...
[2m[36m(launch_and_fit pid=1228433)[0m malloc_consolidate(): invalid chunk size
[2m[36m(launch_and_fit pid=1228436)[0m malloc_consolidate(): invalid chunk size
E0612 23:13:11.608893590 1243090 chttp2_transport.cc:1079]             ipv4:145.136.62.9:46757: Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to "too_many_pings". Current keepalive time (before throttling): ∞
slurmstepd: error: *** JOB 2914012 ON gcn9 CANCELLED AT 2023-06-12T23:13:25 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 2914012.1 ON gcn9 CANCELLED AT 2023-06-12T23:13:25 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.

JOB STATISTICS
==============
Job ID: 2914012
Cluster: snellius
User/Group: sunnys/sunnys
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 03:09:09
CPU Efficiency: 8.75% of 1-12:00:36 core-walltime
Job Wall-clock time: 02:00:02
Memory Utilized: 52.87 GB
Memory Efficiency: 0.00% of 0.00 MB
